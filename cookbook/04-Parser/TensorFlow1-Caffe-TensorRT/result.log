2022-08-11 01:52:29.747444, batch  10, acc = 0.062500
2022-08-11 01:52:31.960219, batch  20, acc = 0.398438
2022-08-11 01:52:33.902738, batch  30, acc = 0.632812
2022-08-11 01:52:35.248815, batch  40, acc = 0.890625
2022-08-11 01:52:36.432820, batch  50, acc = 0.882812
2022-08-11 01:52:36.557514, batch  60, acc = 0.914062
2022-08-11 01:52:36.660353, batch  70, acc = 0.906250
2022-08-11 01:52:36.765405, batch  80, acc = 0.953125
2022-08-11 01:52:36.861096, batch  90, acc = 0.960938
2022-08-11 01:52:36.952507, batch 100, acc = 0.960938
Succeeded building model in TensorFlow1!

WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:tensorflow:From /home/wili/software/anaconda3/envs/py36/lib/python3.6/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:114: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.

WARNING:tensorflow:From /home/wili/software/anaconda3/envs/py36/lib/python3.6/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:114: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.

WARNING:tensorflow:From /home/wili/software/anaconda3/envs/py36/lib/python3.6/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:269: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
WARNING:tensorflow:From /home/wili/software/anaconda3/envs/py36/lib/python3.6/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:269: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
2022-08-11 09:53:57.317604: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fold_constants
WARNING:tensorflow:From /home/wili/software/anaconda3/envs/py36/lib/python3.6/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:305: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/wili/software/anaconda3/envs/py36/lib/python3.6/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:305: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/wili/software/anaconda3/envs/py36/lib/python3.6/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:310: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/wili/software/anaconda3/envs/py36/lib/python3.6/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:310: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2022-08-11 09:53:57.321563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-08-11 09:53:57.346694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-11 09:53:57.346832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: NVIDIA GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:01:00.0
2022-08-11 09:53:57.346933: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/tensorrt-8.4.1.5-ga/lib:/usr/local/sld2/lib:/usr/local/ffmpeg/lib
2022-08-11 09:53:57.346999: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/tensorrt-8.4.1.5-ga/lib:/usr/local/sld2/lib:/usr/local/ffmpeg/lib
2022-08-11 09:53:57.347081: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/tensorrt-8.4.1.5-ga/lib:/usr/local/sld2/lib:/usr/local/ffmpeg/lib
2022-08-11 09:53:57.347129: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/tensorrt-8.4.1.5-ga/lib:/usr/local/sld2/lib:/usr/local/ffmpeg/lib
2022-08-11 09:53:57.347177: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/tensorrt-8.4.1.5-ga/lib:/usr/local/sld2/lib:/usr/local/ffmpeg/lib
2022-08-11 09:53:57.347224: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/tensorrt-8.4.1.5-ga/lib:/usr/local/sld2/lib:/usr/local/ffmpeg/lib
2022-08-11 09:53:57.347273: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/tensorrt-8.4.1.5-ga/lib:/usr/local/sld2/lib:/usr/local/ffmpeg/lib
2022-08-11 09:53:57.347282: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2022-08-11 09:53:57.347456: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2022-08-11 09:53:57.368186: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4200000000 Hz
2022-08-11 09:53:57.368497: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558b5ab4f490 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-08-11 09:53:57.368527: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-08-11 09:53:57.424409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-11 09:53:57.424619: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558b59dccc10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-08-11 09:53:57.424638: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1
2022-08-11 09:53:57.424697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-08-11 09:53:57.424705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      
WARNING:tensorflow:From /home/wili/software/anaconda3/envs/py36/lib/python3.6/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:312: The name tf.train.export_meta_graph is deprecated. Please use tf.compat.v1.train.export_meta_graph instead.

WARNING:tensorflow:From /home/wili/software/anaconda3/envs/py36/lib/python3.6/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:312: The name tf.train.export_meta_graph is deprecated. Please use tf.compat.v1.train.export_meta_graph instead.

WARNING: Logging before InitGoogleLogging() is written to STDERR
I0811 09:53:57.855484  5780 net.cpp:51] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
}
layer {
  name: "Placeholder"
  type: "Input"
  top: "Placeholder"
  input_param {
    shape {
      dim: 1
      dim: 1
      dim: 28
      dim: 28
    }
  }
}
layer {
  name: "Conv2D"
  type: "Convolution"
  bottom: "Placeholder"
  top: "Conv2D"
  convolution_param {
    num_output: 32
    bias_term: false
    group: 1
    stride: 1
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
  }
}
layer {
  name: "Relu"
  type: "ReLU"
  bottom: "Conv2D"
  top: "Conv2D"
}
layer {
  name: "MaxPool2d"
  type: "Pooling"
  bottom: "Conv2D"
  top: "MaxPool2d"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad_h: 0
    pad_w: 0
  }
}
layer {
  name: "Conv2D_1"
  type: "Convolution"
  bottom: "MaxPool2d"
  top: "Conv2D_1"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    stride: 1
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
  }
}
layer {
  name: "Relu_1"
  type: "ReLU"
  bottom: "Conv2D_1"
  top: "Conv2D_1"
}
layer {
  name: "MaxPool2d_1"
  type: "Pooling"
  bottom: "Conv2D_1"
  top: "MaxPool2d_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad_h: 0
    pad_w: 0
  }
}
layer {
  name: "Reshape"
  type: "Reshape"
  bottom: "MaxPool2d_1"
  top: "Reshape"
  reshape_param {
    shape {
      dim: 1
      dim: 3136
    }
  }
}
layer {
  name: "MatMul"
  type: "InnerProduct"
  bottom: "Reshape"
  top: "MatMul"
  inner_product_param {
    num_output: 1024
    bias_term: false
  }
}
layer {
  name: "Relu_2"
  type: "ReLU"
  bottom: "MatMul"
  top: "MatMul"
}
layer {
  name: "MatMul_1"
  type: "InnerProduct"
  bottom: "MatMul"
  top: "MatMul_1"
  inner_product_param {
    num_output: 10
    bias_term: false
  }
}
layer {
  name: "y"
  type: "Softmax"
  bottom: "MatMul_1"
  top: "y"
}
I0811 09:53:57.855579  5780 layer_factory.hpp:77] Creating layer Placeholder
I0811 09:53:57.855592  5780 net.cpp:84] Creating Layer Placeholder
I0811 09:53:57.855602  5780 net.cpp:380] Placeholder -> Placeholder
I0811 09:53:57.855628  5780 net.cpp:122] Setting up Placeholder
I0811 09:53:57.855648  5780 net.cpp:129] Top shape: 1 1 28 28 (784)
I0811 09:53:57.855654  5780 net.cpp:137] Memory required for data: 3136
I0811 09:53:57.855660  5780 layer_factory.hpp:77] Creating layer Conv2D
I0811 09:53:57.855685  5780 net.cpp:84] Creating Layer Conv2D
I0811 09:53:57.855691  5780 net.cpp:406] Conv2D <- Placeholder
I0811 09:53:57.855710  5780 net.cpp:380] Conv2D -> Conv2D
I0811 09:53:57.855741  5780 net.cpp:122] Setting up Conv2D
I0811 09:53:57.855747  5780 net.cpp:129] Top shape: 1 32 28 28 (25088)
I0811 09:53:57.855767  5780 net.cpp:137] Memory required for data: 103488
I0811 09:53:57.855775  5780 layer_factory.hpp:77] Creating layer Relu
I0811 09:53:57.855798  5780 net.cpp:84] Creating Layer Relu
I0811 09:53:57.855803  5780 net.cpp:406] Relu <- Conv2D
I0811 09:53:57.855808  5780 net.cpp:367] Relu -> Conv2D (in-place)
I0811 09:53:57.855818  5780 net.cpp:122] Setting up Relu
I0811 09:53:57.855824  5780 net.cpp:129] Top shape: 1 32 28 28 (25088)
I0811 09:53:57.855830  5780 net.cpp:137] Memory required for data: 203840
I0811 09:53:57.855835  5780 layer_factory.hpp:77] Creating layer MaxPool2d
I0811 09:53:57.855842  5780 net.cpp:84] Creating Layer MaxPool2d
I0811 09:53:57.855849  5780 net.cpp:406] MaxPool2d <- Conv2D
I0811 09:53:57.855854  5780 net.cpp:380] MaxPool2d -> MaxPool2d
I0811 09:53:57.855865  5780 net.cpp:122] Setting up MaxPool2d
I0811 09:53:57.855871  5780 net.cpp:129] Top shape: 1 32 14 14 (6272)
I0811 09:53:57.855877  5780 net.cpp:137] Memory required for data: 228928
I0811 09:53:57.855882  5780 layer_factory.hpp:77] Creating layer Conv2D_1
I0811 09:53:57.855888  5780 net.cpp:84] Creating Layer Conv2D_1
I0811 09:53:57.855895  5780 net.cpp:406] Conv2D_1 <- MaxPool2d
I0811 09:53:57.855901  5780 net.cpp:380] Conv2D_1 -> Conv2D_1
I0811 09:53:57.855931  5780 net.cpp:122] Setting up Conv2D_1
I0811 09:53:57.855938  5780 net.cpp:129] Top shape: 1 64 14 14 (12544)
I0811 09:53:57.855944  5780 net.cpp:137] Memory required for data: 279104
I0811 09:53:57.855958  5780 layer_factory.hpp:77] Creating layer Relu_1
I0811 09:53:57.855966  5780 net.cpp:84] Creating Layer Relu_1
I0811 09:53:57.855973  5780 net.cpp:406] Relu_1 <- Conv2D_1
I0811 09:53:57.855978  5780 net.cpp:367] Relu_1 -> Conv2D_1 (in-place)
I0811 09:53:57.855985  5780 net.cpp:122] Setting up Relu_1
I0811 09:53:57.855990  5780 net.cpp:129] Top shape: 1 64 14 14 (12544)
I0811 09:53:57.855998  5780 net.cpp:137] Memory required for data: 329280
I0811 09:53:57.856002  5780 layer_factory.hpp:77] Creating layer MaxPool2d_1
I0811 09:53:57.856009  5780 net.cpp:84] Creating Layer MaxPool2d_1
I0811 09:53:57.856014  5780 net.cpp:406] MaxPool2d_1 <- Conv2D_1
I0811 09:53:57.856020  5780 net.cpp:380] MaxPool2d_1 -> MaxPool2d_1
I0811 09:53:57.856029  5780 net.cpp:122] Setting up MaxPool2d_1
I0811 09:53:57.856035  5780 net.cpp:129] Top shape: 1 64 7 7 (3136)
I0811 09:53:57.856041  5780 net.cpp:137] Memory required for data: 341824
I0811 09:53:57.856046  5780 layer_factory.hpp:77] Creating layer Reshape
I0811 09:53:57.856056  5780 net.cpp:84] Creating Layer Reshape
I0811 09:53:57.856062  5780 net.cpp:406] Reshape <- MaxPool2d_1
I0811 09:53:57.856068  5780 net.cpp:380] Reshape -> Reshape
I0811 09:53:57.856077  5780 net.cpp:122] Setting up Reshape
I0811 09:53:57.856083  5780 net.cpp:129] Top shape: 1 3136 (3136)
I0811 09:53:57.856089  5780 net.cpp:137] Memory required for data: 354368
I0811 09:53:57.856094  5780 layer_factory.hpp:77] Creating layer MatMul
I0811 09:53:57.856101  5780 net.cpp:84] Creating Layer MatMul
I0811 09:53:57.856107  5780 net.cpp:406] MatMul <- Reshape
I0811 09:53:57.856114  5780 net.cpp:380] MatMul -> MatMul
I0811 09:53:57.859835  5780 net.cpp:122] Setting up MatMul
I0811 09:53:57.859863  5780 net.cpp:129] Top shape: 1 1024 (1024)
I0811 09:53:57.859869  5780 net.cpp:137] Memory required for data: 358464
I0811 09:53:57.859879  5780 layer_factory.hpp:77] Creating layer Relu_2
I0811 09:53:57.859889  5780 net.cpp:84] Creating Layer Relu_2
I0811 09:53:57.859894  5780 net.cpp:406] Relu_2 <- MatMul
I0811 09:53:57.859901  5780 net.cpp:367] Relu_2 -> MatMul (in-place)
I0811 09:53:57.859910  5780 net.cpp:122] Setting up Relu_2
I0811 09:53:57.859917  5780 net.cpp:129] Top shape: 1 1024 (1024)
I0811 09:53:57.859923  5780 net.cpp:137] Memory required for data: 362560
I0811 09:53:57.859928  5780 layer_factory.hpp:77] Creating layer MatMul_1
I0811 09:53:57.859934  5780 net.cpp:84] Creating Layer MatMul_1
I0811 09:53:57.859941  5780 net.cpp:406] MatMul_1 <- MatMul
I0811 09:53:57.859948  5780 net.cpp:380] MatMul_1 -> MatMul_1
I0811 09:53:57.859983  5780 net.cpp:122] Setting up MatMul_1
I0811 09:53:57.859989  5780 net.cpp:129] Top shape: 1 10 (10)
I0811 09:53:57.859994  5780 net.cpp:137] Memory required for data: 362600
I0811 09:53:57.860002  5780 layer_factory.hpp:77] Creating layer y
I0811 09:53:57.860008  5780 net.cpp:84] Creating Layer y
I0811 09:53:57.860015  5780 net.cpp:406] y <- MatMul_1
I0811 09:53:57.860021  5780 net.cpp:380] y -> y
I0811 09:53:57.860036  5780 net.cpp:122] Setting up y
I0811 09:53:57.860041  5780 net.cpp:129] Top shape: 1 10 (10)
I0811 09:53:57.860047  5780 net.cpp:137] Memory required for data: 362640
I0811 09:53:57.860052  5780 net.cpp:200] y does not need backward computation.
I0811 09:53:57.860059  5780 net.cpp:200] MatMul_1 does not need backward computation.
I0811 09:53:57.860064  5780 net.cpp:200] Relu_2 does not need backward computation.
I0811 09:53:57.860070  5780 net.cpp:200] MatMul does not need backward computation.
I0811 09:53:57.860075  5780 net.cpp:200] Reshape does not need backward computation.
I0811 09:53:57.860081  5780 net.cpp:200] MaxPool2d_1 does not need backward computation.
I0811 09:53:57.860086  5780 net.cpp:200] Relu_1 does not need backward computation.
I0811 09:53:57.860091  5780 net.cpp:200] Conv2D_1 does not need backward computation.
I0811 09:53:57.860097  5780 net.cpp:200] MaxPool2d does not need backward computation.
I0811 09:53:57.860110  5780 net.cpp:200] Relu does not need backward computation.
I0811 09:53:57.860116  5780 net.cpp:200] Conv2D does not need backward computation.
I0811 09:53:57.860121  5780 net.cpp:200] Placeholder does not need backward computation.
I0811 09:53:57.860126  5780 net.cpp:242] This network produces output y
I0811 09:53:57.860136  5780 net.cpp:255] Network initialization done.
Parse file [./model.ckpt.meta] with binary format successfully.
Tensorflow model file [./model.ckpt.meta] loaded successfully.
Tensorflow checkpoint file [./model.ckpt] loaded successfully. [18] variables loaded.
IR network structure is saved as [c442065daeb94f4090a9b12c8c1b842a.json].
IR network structure is saved as [c442065daeb94f4090a9b12c8c1b842a.pb].
IR weights are saved as [c442065daeb94f4090a9b12c8c1b842a.npy].
Parse file [c442065daeb94f4090a9b12c8c1b842a.pb] with binary format successfully.
Target network code snippet is saved as [c442065daeb94f4090a9b12c8c1b842a.py].
Target weights are saved as [c442065daeb94f4090a9b12c8c1b842a.npy].
Caffe model files are saved as [model.prototxt] and [model.caffemodel], generated by [c442065daeb94f4090a9b12c8c1b842a.py] and [c442065daeb94f4090a9b12c8c1b842a.npy].
Warning, setting batch size to 1. Update the dimension after parsing due to using explicit batch size.
Succeeded finding caffe file!
Succeeded parsing cafe file!
Succeeded building engine!
inputH0 : (1, 1, 28, 28)
outputH0: (1, 1)
[[8]]
Succeeded running model in TensorRT!
