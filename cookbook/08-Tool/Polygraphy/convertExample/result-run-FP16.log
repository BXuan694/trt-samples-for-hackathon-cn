[08/13/2022-15:48:45] [TRT] [I] [MemUsageChange] Init CUDA: CPU +195, GPU +0, now: CPU 206, GPU 938 (MiB)
[08/13/2022-15:48:46] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +7, GPU +2, now: CPU 232, GPU 940 (MiB)
[08/13/2022-15:48:46] [TRT] [I] ----------------------------------------------------------------
[08/13/2022-15:48:46] [TRT] [I] Input filename:   /work/gitlab/tensorrt-cookbook-in-chinese/08-Tool/Polygraphy/convertExample/model.onnx
[08/13/2022-15:48:46] [TRT] [I] ONNX IR version:  0.0.8
[08/13/2022-15:48:46] [TRT] [I] Opset version:    11
[08/13/2022-15:48:46] [TRT] [I] Producer name:    
[08/13/2022-15:48:46] [TRT] [I] Producer version: 
[08/13/2022-15:48:46] [TRT] [I] Domain:           
[08/13/2022-15:48:46] [TRT] [I] Model version:    0
[08/13/2022-15:48:46] [TRT] [I] Doc string:       
[08/13/2022-15:48:46] [TRT] [I] ----------------------------------------------------------------
[08/13/2022-15:48:46] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:367: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
[08/13/2022-15:48:46] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.
[08/13/2022-15:48:46] [TRT] [W] FP16 support requested on hardware without native FP16 support, performance will be negatively affected.
[08/13/2022-15:48:46] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +255, GPU +106, now: CPU 512, GPU 1046 (MiB)
[08/13/2022-15:48:46] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +113, GPU +46, now: CPU 625, GPU 1092 (MiB)
[08/13/2022-15:48:46] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.
[08/13/2022-15:48:47] [TRT] [W] Weights [name=Conv-4 + ReLU-5.weight] had the following issues when converted to FP16:
[08/13/2022-15:48:47] [TRT] [W]  - Subnormal FP16 values detected. 
[08/13/2022-15:48:47] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/13/2022-15:48:47] [TRT] [W] Weights [name=Conv-4 + ReLU-5.weight] had the following issues when converted to FP16:
[08/13/2022-15:48:47] [TRT] [W]  - Subnormal FP16 values detected. 
[08/13/2022-15:48:47] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/13/2022-15:48:47] [TRT] [I] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.
[08/13/2022-15:48:48] [TRT] [W] Weights [name=MatMul-9 + ReLU-11.weight] had the following issues when converted to FP16:
[08/13/2022-15:48:48] [TRT] [W]  - Subnormal FP16 values detected. 
[08/13/2022-15:48:48] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/13/2022-15:48:48] [TRT] [W] Weights [name=MatMul-9 + ReLU-11.weight] had the following issues when converted to FP16:
[08/13/2022-15:48:48] [TRT] [W]  - Subnormal FP16 values detected. 
[08/13/2022-15:48:48] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/13/2022-15:48:48] [TRT] [W] Weights [name=MatMul-9 + ReLU-11.weight] had the following issues when converted to FP16:
[08/13/2022-15:48:48] [TRT] [W]  - Subnormal FP16 values detected. 
[08/13/2022-15:48:48] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/13/2022-15:48:48] [TRT] [W] Weights [name=MatMul-12.weight] had the following issues when converted to FP16:
[08/13/2022-15:48:48] [TRT] [W]  - Subnormal FP16 values detected. 
[08/13/2022-15:48:48] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/13/2022-15:48:48] [TRT] [W] Weights [name=MatMul-12.weight] had the following issues when converted to FP16:
[08/13/2022-15:48:48] [TRT] [W]  - Subnormal FP16 values detected. 
[08/13/2022-15:48:48] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/13/2022-15:48:48] [TRT] [W] Weights [name=MatMul-12.weight] had the following issues when converted to FP16:
[08/13/2022-15:48:48] [TRT] [W]  - Subnormal FP16 values detected. 
[08/13/2022-15:48:48] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.
[08/13/2022-15:48:48] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[08/13/2022-15:48:48] [TRT] [I] Total Host Persistent Memory: 4032
[08/13/2022-15:48:48] [TRT] [I] Total Device Persistent Memory: 6656
[08/13/2022-15:48:48] [TRT] [I] Total Scratch Memory: 0
[08/13/2022-15:48:48] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 24 MiB, GPU 438 MiB
[08/13/2022-15:48:48] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.050808ms to assign 4 blocks to 12 nodes requiring 2408456 bytes.
[08/13/2022-15:48:48] [TRT] [I] Total Activation Memory: 2408456
[08/13/2022-15:48:48] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 926, GPU 1232 (MiB)
[08/13/2022-15:48:48] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +12, GPU +13, now: CPU 12, GPU 13 (MiB)
[08/13/2022-15:48:48] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[08/13/2022-15:48:48] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[08/13/2022-15:48:48] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 925, GPU 1192 (MiB)
[08/13/2022-15:48:48] [TRT] [I] Loaded engine size: 12 MiB
[08/13/2022-15:48:48] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 926, GPU 1216 (MiB)
[08/13/2022-15:48:48] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12, now: CPU 0, GPU 12 (MiB)
[08/13/2022-15:48:48] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[08/13/2022-15:48:48] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[V] Loaded Module: polygraphy.util    | Path: ['/opt/conda/lib/python3.8/site-packages/polygraphy/util']
[V] Model: model.onnx
[V] Loaded Module: polygraphy         | Version: 0.38.0 | Path: ['/opt/conda/lib/python3.8/site-packages/polygraphy']
[V] Loaded Module: tensorrt           | Version: 8.4.1.5 | Path: ['/opt/conda/lib/python3.8/site-packages/tensorrt']
[V]     Setting TensorRT Optimization Profiles
[V]     Input tensor: tensor-0 (dtype=DataType.FLOAT, shape=(-1, 1, 28, 28)) | Setting input tensor shapes to: (min=[1, 1, 28, 28], opt=[4, 1, 28, 28], max=[16, 1, 28, 28])
[I]     Configuring with profiles: [Profile().add('tensor-0', min=[1, 1, 28, 28], opt=[4, 1, 28, 28], max=[16, 1, 28, 28])]
[I] Building engine with configuration:
    Workspace            | 1000000000 bytes (953.67 MiB)
    Precision            | TF32: False, FP16: True, INT8: False, Obey Precision Constraints: False, Strict Types: False
    Tactic Sources       | ['CUBLAS', 'CUBLAS_LT', 'CUDNN', 'EDGE_MASK_CONVOLUTIONS']
    Safety Restricted    | False
    Profiles             | 1 profile(s)
[I] Finished engine building in 2.187 seconds
[I] Saving engine to ./model-FP16.plan
