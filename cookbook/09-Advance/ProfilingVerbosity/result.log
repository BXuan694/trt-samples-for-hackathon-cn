[08/18/2022-12:25:05] [TRT] [I] [MemUsageChange] Init CUDA: CPU +188, GPU +0, now: CPU 209, GPU 1008 (MiB)
[08/18/2022-12:25:06] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +6, GPU +2, now: CPU 234, GPU 1010 (MiB)
[08/18/2022-12:25:06] [TRT] [V] Applying generic optimizations to the graph for inference.
[08/18/2022-12:25:06] [TRT] [V] Original: 18 layers
[08/18/2022-12:25:06] [TRT] [V] After dead-layer removal: 18 layers
[08/18/2022-12:25:06] [TRT] [V] After Myelin optimization: 18 layers
[08/18/2022-12:25:06] [TRT] [V] Running: MatMulToConvTransform on (Unnamed Layer* 8) [Matrix Multiply]
[08/18/2022-12:25:06] [TRT] [V] Convert layer type of (Unnamed Layer* 8) [Matrix Multiply] from MATRIX_MULTIPLY to CONVOLUTION
[08/18/2022-12:25:06] [TRT] [V] Running: MatMulToConvTransform on (Unnamed Layer* 13) [Matrix Multiply]
[08/18/2022-12:25:06] [TRT] [V] Convert layer type of (Unnamed Layer* 13) [Matrix Multiply] from MATRIX_MULTIPLY to CONVOLUTION
[08/18/2022-12:25:06] [TRT] [V] Running: ShuffleShuffleFusion on (Unnamed Layer* 6) [Shuffle]
[08/18/2022-12:25:06] [TRT] [V] ShuffleShuffleFusion: Fusing (Unnamed Layer* 6) [Shuffle] with reshape_before_(Unnamed Layer* 8) [Matrix Multiply]
[08/18/2022-12:25:06] [TRT] [V] Running: ConvReshapeBiasAddFusion on (Unnamed Layer* 8) [Matrix Multiply]
[08/18/2022-12:25:06] [TRT] [V] Running: ConvReshapeBiasAddFusion on (Unnamed Layer* 13) [Matrix Multiply]
[08/18/2022-12:25:06] [TRT] [V] Applying ScaleNodes fusions.
[08/18/2022-12:25:06] [TRT] [V] After scale fusion: 15 layers
[08/18/2022-12:25:06] [TRT] [V] Running: SqueezePushDownFork on reshape_after_(Unnamed Layer* 8) [Matrix Multiply]
[08/18/2022-12:25:06] [TRT] [V] -----------SqueezePushDown kSQUEEZE_FORK case: (Unnamed Layer* 8) [Matrix Multiply] --> reshape_after_(Unnamed Layer* 8) [Matrix Multiply] --> (Unnamed Layer* 11) [Activation]
[08/18/2022-12:25:06] [TRT] [V] Running: ShuffleShuffleFusion on squeeze_after_(Unnamed Layer* 11) [Activation]
[08/18/2022-12:25:06] [TRT] [V] ShuffleShuffleFusion: Fusing squeeze_after_(Unnamed Layer* 11) [Activation] with reshape_before_(Unnamed Layer* 13) [Matrix Multiply]
[08/18/2022-12:25:06] [TRT] [V] Running: ConvReluFusion on (Unnamed Layer* 0) [Convolution]
[08/18/2022-12:25:06] [TRT] [V] ConvReluFusion: Fusing (Unnamed Layer* 0) [Convolution] with (Unnamed Layer* 1) [Activation]
[08/18/2022-12:25:06] [TRT] [V] Running: ConvReluFusion on (Unnamed Layer* 3) [Convolution]
[08/18/2022-12:25:06] [TRT] [V] ConvReluFusion: Fusing (Unnamed Layer* 3) [Convolution] with (Unnamed Layer* 4) [Activation]
[08/18/2022-12:25:06] [TRT] [V] Running: ConvReluFusion on (Unnamed Layer* 8) [Matrix Multiply]
[08/18/2022-12:25:06] [TRT] [V] ConvReluFusion: Fusing (Unnamed Layer* 8) [Matrix Multiply] with (Unnamed Layer* 11) [Activation]
[08/18/2022-12:25:06] [TRT] [V] Running: ShuffleErasure on squeeze_after_(Unnamed Layer* 11) [Activation] + reshape_before_(Unnamed Layer* 13) [Matrix Multiply]
[08/18/2022-12:25:06] [TRT] [V] Removing squeeze_after_(Unnamed Layer* 11) [Activation] + reshape_before_(Unnamed Layer* 13) [Matrix Multiply]
[08/18/2022-12:25:06] [TRT] [V] Running: SoftmaxTopKFusion on (Unnamed Layer* 16) [Softmax]
[08/18/2022-12:25:06] [TRT] [V] SoftmaxTopKFusion: Fusing (Unnamed Layer* 16) [Softmax] with (Unnamed Layer* 17) [TopK]
[08/18/2022-12:25:06] [TRT] [V] After dupe layer removal: 9 layers
[08/18/2022-12:25:06] [TRT] [V] After final dead-layer removal: 9 layers
[08/18/2022-12:25:06] [TRT] [V] After tensor merging: 9 layers
[08/18/2022-12:25:06] [TRT] [V] After vertical fusions: 9 layers
[08/18/2022-12:25:06] [TRT] [V] After dupe layer removal: 9 layers
[08/18/2022-12:25:06] [TRT] [V] After final dead-layer removal: 9 layers
[08/18/2022-12:25:06] [TRT] [V] After tensor merging: 9 layers
[08/18/2022-12:25:06] [TRT] [V] After slice removal: 9 layers
[08/18/2022-12:25:06] [TRT] [V] After concat removal: 9 layers
[08/18/2022-12:25:06] [TRT] [V] Trying to split Reshape and strided tensor
[08/18/2022-12:25:06] [TRT] [V] Graph construction and optimization completed in 0.031465 seconds.
[08/18/2022-12:25:06] [TRT] [V] Using cublas as a tactic source
[08/18/2022-12:25:06] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +256, GPU +106, now: CPU 515, GPU 1116 (MiB)
[08/18/2022-12:25:06] [TRT] [V] Using cuDNN as a tactic source
[08/18/2022-12:25:06] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +113, GPU +46, now: CPU 628, GPU 1162 (MiB)
[08/18/2022-12:25:06] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[08/18/2022-12:25:06] [TRT] [V] Constructing optimization profile number 0 [1/1].
[08/18/2022-12:25:06] [TRT] [V] Reserving memory for host IO tensors. Host: 0 bytes
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(784,784,28,1) -> Float(784,1,28,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat(inputT0 -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00251501
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.00535043
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00246969
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00246969
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(25088,1,896,32) -> Float(25088,784,28,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 1) [Activation]_output -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00725704
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.016449
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00731501
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x00000000000003e8 Time: 0.00725704
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(6272,196,14,1) -> Float(6272,1,448,32) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 2) [Pooling]_output) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00400559
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0159609
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00397613
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00397613
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(6272,196,14,1) -> Float(6272,1,448,32) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 2) [Pooling]_output -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00401696
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0159289
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00401219
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00401219
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(6272,1,448,32) -> Float(6272,196,14,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 2) [Pooling]_output -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00391678
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0157896
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00391355
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00391355
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(12544,1,896,64) -> Float(12544,196,14,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 4) [Activation]_output -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00502097
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.015377
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00496486
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00496486
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(3136,49,7,1) -> Float(3136,1,448,64) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 5) [Pooling]_output) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00325768
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0145693
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00329161
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x00000000000003e8 Time: 0.00325768
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(3136,49,7,1) -> Float(98,49:32,7,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 5) [Pooling]_output) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00369067
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0145141
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00366319
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00366319
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(3136,49,7,1) -> Float(3136,1,448,64) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 5) [Pooling]_output -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.003258
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0144778
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00328209
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x00000000000003e8 Time: 0.003258
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(3136,49,7,1) -> Float(98,49:32,7,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 5) [Pooling]_output -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00368
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0146031
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00364847
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00364847
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(3136,1,448,64) -> Float(3136,49,7,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 5) [Pooling]_output -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00316317
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0145238
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00326732
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x00000000000003e8 Time: 0.00316317
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(3136,1,448,64) -> Float(98,49:32,7,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 5) [Pooling]_output -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00392722
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0116921
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.0038503
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.0038503
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(98,49:32,7,1) -> Float(3136,49,7,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 5) [Pooling]_output -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.0031809
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0145141
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00323231
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x00000000000003e8 Time: 0.0031809
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(98,49:32,7,1) -> Float(3136,1,448,64) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 5) [Pooling]_output -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00319289
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0117043
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00313999
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00313999
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(3136,1,1,1) -> Float(3136,1,3136,3136) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat(reshape_before_(Unnamed Layer* 8) [Matrix Multiply]_out_tensor -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00281788
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.00745884
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00281959
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x00000000000003e8 Time: 0.00281788
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(3136,1,3136,3136) -> Float(3136,1,1,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat(reshape_before_(Unnamed Layer* 8) [Matrix Multiply]_out_tensor -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00276884
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.00757167
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00277016
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x00000000000003e8 Time: 0.00276884
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(98,1:32,1,1) -> Float(3136,1,1,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat(reshape_before_(Unnamed Layer* 8) [Matrix Multiply]_out_tensor -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00295335
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0406187
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00319645
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x00000000000003e8 Time: 0.00295335
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(98,1:32,1,1) -> Float(3136,1,3136,3136) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat(reshape_before_(Unnamed Layer* 8) [Matrix Multiply]_out_tensor -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00304669
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0141796
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00300704
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00300704
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(1024,1,1,1) -> Float(1024,1,1024,1024) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 11) [Activation]_out_tensor -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00249051
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.00763151
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00250883
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x00000000000003e8 Time: 0.00249051
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(1024,1,1024,1024) -> Float(1024,1,1,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 11) [Activation]_out_tensor -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.00263635
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.00757428
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00248083
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00248083
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning Reformat: Float(10,1,10,10) -> Float(10,1,1,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 13) [Matrix Multiply]_out_tensor -> <out>) (Reformat)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003e8 Time: 0.0023888
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0132985
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00281896
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x00000000000003e8 Time: 0.0023888
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] =============== Computing reformatting costs
[08/18/2022-12:25:06] [TRT] [V] =============== Computing costs for 
[08/18/2022-12:25:06] [TRT] [V] *************** Autotuning format combination: Float(784,784,28,1) -> Float(25088,784,28,1) ***************
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] (CudaDepthwiseConvolution)
[08/18/2022-12:25:06] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] (FusedConvActConvolution)
[08/18/2022-12:25:06] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] (CudnnConvolution)
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.0229376
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000001 Time: 0.0208364
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000002 Time: 0.0277694
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000004 Time: 0.0415953
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0000000000000005 Time: 0.0416794
[08/18/2022-12:25:06] [TRT] [V] Fastest Tactic: 0x0000000000000001 Time: 0.0208364
[08/18/2022-12:25:06] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] (CaskConvolution)
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1 Tactic: 0x0ebe499388e08286
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x0ebe499388e08286 Time: 0.011886
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v0 Tactic: 0x185af5379580418f
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x185af5379580418f Time: 0.0092356
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v0 Tactic: 0x321f7a577fb21da0
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x321f7a577fb21da0 Time: 0.0160102
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1 Tactic: 0x3c301f1cd57bd89b
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x3c301f1cd57bd89b Time: 0.0121417
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x3e787008e11a6129
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x3e787008e11a6129 Time: 0.0152669
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1 Tactic: 0x474c9edd1ecfbbba
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x474c9edd1ecfbbba Time: 0.0109526
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v0 Tactic: 0x4963fb96b4067e81
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x4963fb96b4067e81 Time: 0.0153445
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x5c38385751ccb068
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x5c38385751ccb068 Time: 0.011787
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v0 Tactic: 0x632674f65e3422ae
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x632674f65e3422ae Time: 0.00853227
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1 Tactic: 0x813136e97c1542cf
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x813136e97c1542cf Time: 0.0157168
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v0 Tactic: 0x8d563cb6e2bd3e46
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x8d563cb6e2bd3e46 Time: 0.0158608
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v0 Tactic: 0x8f1e53a2d6dc87f4
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0x8f1e53a2d6dc87f4 Time: 0.0120716
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1 Tactic: 0xab74b98996271ee0
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0xab74b98996271ee0 Time: 0.012431
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v0 Tactic: 0xbd90052d8b47dde9
[08/18/2022-12:25:06] [TRT] [V] Tactic: 0xbd90052d8b47dde9 Time: 0.00909866
[08/18/2022-12:25:06] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1 Tactic: 0xd00838485d937dc1
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xd00838485d937dc1 Time: 0.010427
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v0 Tactic: 0xef1674e9526bef07
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xef1674e9526bef07 Time: 0.0116885
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v0 Tactic: 0xf462d2631d68e4d5
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xf462d2631d68e4d5 Time: 0.0113895
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1 Tactic: 0xfa4db728b7a121ee
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xfa4db728b7a121ee Time: 0.0147557
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x632674f65e3422ae Time: 0.00853227
[08/18/2022-12:25:07] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x632674f65e3422ae
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(784,1,28,1) -> Float(25088,1,896,32) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] (CaskConvolution)
[08/18/2022-12:25:07] [TRT] [V] CaskConvolution has no valid tactics for this config, skipping
[08/18/2022-12:25:07] [TRT] [V] =============== Computing costs for 
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(25088,784,28,1) -> Float(6272,196,14,1) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 2) [Pooling] (TiledPooling)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000540101 Time: 0.00814205
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000550101 Time: 0.00564338
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000560101 Time: 0.00497522
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000570101 Time: 0.00457045
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000580101 Time: 0.00454803
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000590101 Time: 0.00474516
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000005a0101 Time: 0.00480305
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000005b0101 Time: 0.00397041
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000005c0101 Time: 0.00526267
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000005d0101 Time: 0.00388031
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000005e0101 Time: 0.00370916
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000005f0101 Time: 0.00336555
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000600101 Time: 0.00342346
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000610101 Time: 0.00331829
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000620101 Time: 0.00329391
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000630101 Time: 0.00320366
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x0000000000630101 Time: 0.00320366
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 2) [Pooling] (CudnnPooling)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xffffffffffffffff Time: 0.00381467
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0xffffffffffffffff Time: 0.00381467
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 2) [Pooling] (CaskPooling)
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 2) [Pooling] Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Max Tactic: 0xb59f9cfb90407c92
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xb59f9cfb90407c92 Time: 0.00401498
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0xb59f9cfb90407c92 Time: 0.00401498
[08/18/2022-12:25:07] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: TiledPooling Tactic: 0x0000000000630101
[08/18/2022-12:25:07] [TRT] [V] =============== Computing costs for 
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(6272,196,14,1) -> Float(12544,196,14,1) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] (CudaDepthwiseConvolution)
[08/18/2022-12:25:07] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] (FusedConvActConvolution)
[08/18/2022-12:25:07] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] (CudnnConvolution)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.0767573
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000001 Time: 0.0794624
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000002 Time: 0.119922
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000004 Time: 0.187563
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000005 Time: 0.195355
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.0767573
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] (CaskConvolution)
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1 Tactic: 0x0ebe499388e08286
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0ebe499388e08286 Time: 0.122766
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v0 Tactic: 0x185af5379580418f
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x185af5379580418f Time: 0.0899413
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v0 Tactic: 0x321f7a577fb21da0
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x321f7a577fb21da0 Time: 0.0987307
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1 Tactic: 0x3c301f1cd57bd89b
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x3c301f1cd57bd89b Time: 0.090624
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x3e787008e11a6129
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x3e787008e11a6129 Time: 0.0989013
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1 Tactic: 0x474c9edd1ecfbbba
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x474c9edd1ecfbbba Time: 0.0595058
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v0 Tactic: 0x4963fb96b4067e81
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x4963fb96b4067e81 Time: 0.0854107
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x5c38385751ccb068
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x5c38385751ccb068 Time: 0.0816832
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v0 Tactic: 0x632674f65e3422ae
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x632674f65e3422ae Time: 0.0686763
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1 Tactic: 0x813136e97c1542cf
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x813136e97c1542cf Time: 0.102144
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v0 Tactic: 0x8d563cb6e2bd3e46
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x8d563cb6e2bd3e46 Time: 0.0984747
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v0 Tactic: 0x8f1e53a2d6dc87f4
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x8f1e53a2d6dc87f4 Time: 0.106155
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1 Tactic: 0xab74b98996271ee0
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xab74b98996271ee0 Time: 0.126869
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v0 Tactic: 0xbd90052d8b47dde9
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xbd90052d8b47dde9 Time: 0.088832
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1 Tactic: 0xd00838485d937dc1
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xd00838485d937dc1 Time: 0.0700416
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v0 Tactic: 0xef1674e9526bef07
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xef1674e9526bef07 Time: 0.105045
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v0 Tactic: 0xf462d2631d68e4d5
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xf462d2631d68e4d5 Time: 0.0863573
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1 Tactic: 0xfa4db728b7a121ee
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xfa4db728b7a121ee Time: 0.083456
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x474c9edd1ecfbbba Time: 0.0595058
[08/18/2022-12:25:07] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x474c9edd1ecfbbba
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(6272,1,448,32) -> Float(12544,1,896,64) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] (CaskConvolution)
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x80f932c0b8ce5940
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x80f932c0b8ce5940 Time: 0.0264139
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x9961ac24fc07a1df
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x9961ac24fc07a1df Time: 0.0466707
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x80f932c0b8ce5940 Time: 0.0264139
[08/18/2022-12:25:07] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x80f932c0b8ce5940
[08/18/2022-12:25:07] [TRT] [V] =============== Computing costs for 
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(12544,196,14,1) -> Float(3136,49,7,1) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 5) [Pooling] (TiledPooling)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000540101 Time: 0.00890162
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000550101 Time: 0.00618371
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000560101 Time: 0.00558827
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000570101 Time: 0.00492972
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000580101 Time: 0.00462743
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000590101 Time: 0.00478644
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000005a0101 Time: 0.0049211
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000005b0101 Time: 0.00429716
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000005c0101 Time: 0.00566961
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000005d0101 Time: 0.00414525
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000005e0101 Time: 0.0037796
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000005f0101 Time: 0.00347422
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000600101 Time: 0.00344039
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000610101 Time: 0.00332547
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000620101 Time: 0.0033822
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000630101 Time: 0.00330741
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x0000000000630101 Time: 0.00330741
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 5) [Pooling] (CudnnPooling)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xffffffffffffffff Time: 0.00547672
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0xffffffffffffffff Time: 0.00547672
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 5) [Pooling] (CaskPooling)
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 5) [Pooling] Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Max Tactic: 0xb59f9cfb90407c92
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xb59f9cfb90407c92 Time: 0.00353774
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0xb59f9cfb90407c92 Time: 0.00353774
[08/18/2022-12:25:07] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: TiledPooling Tactic: 0x0000000000630101
[08/18/2022-12:25:07] [TRT] [V] =============== Computing costs for 
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(3136,49,7,1) -> Float(3136,1,1,1) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 6) [Shuffle] + reshape_before_(Unnamed Layer* 8) [Matrix Multiply] (Shuffle)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00657234
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000001 Time: 0.0152465
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00657234
[08/18/2022-12:25:07] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(3136,1,448,64) -> Float(3136,1,3136,3136) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 6) [Shuffle] + reshape_before_(Unnamed Layer* 8) [Matrix Multiply] (Shuffle)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00485302
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000001 Time: 0.0201794
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00485302
[08/18/2022-12:25:07] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(98,49:32,7,1) -> Float(98,1:32,1,1) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 6) [Shuffle] + reshape_before_(Unnamed Layer* 8) [Matrix Multiply] (Shuffle)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00462217
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000001 Time: 0.0564622
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00462217
[08/18/2022-12:25:07] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000
[08/18/2022-12:25:07] [TRT] [V] =============== Computing costs for 
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(3136,1,1,1) -> Float(1024,1,1,1) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] (CudaDepthwiseConvolution)
[08/18/2022-12:25:07] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] (FusedConvActConvolution)
[08/18/2022-12:25:07] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] (CudnnConvolution)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.316075
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000001 Time: 0.290475
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000002 Time: 0.387755
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000004 skipped. Scratch requested: 7469400064, available: 6442450944
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000005 Time: 6.35529
[08/18/2022-12:25:07] [TRT] [I] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x0000000000000001 Time: 0.290475
[08/18/2022-12:25:07] [TRT] [V] Setting workspace to 7469400064enables more tactics for profiling
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] (CublasConvolution)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.0723627
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000001 Time: 0.0692907
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x0000000000000001 Time: 0.0692907
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] (CaskConvolution)
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1 Tactic: 0x0ebe499388e08286
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0ebe499388e08286 Time: 0.376149
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v0 Tactic: 0x1792ed6b0f1ea883
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x1792ed6b0f1ea883 Time: 0.370005
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x3e787008e11a6129
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x3e787008e11a6129 Time: 0.4096
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1 Tactic: 0x474c9edd1ecfbbba
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x474c9edd1ecfbbba Time: 0.319488
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v0 Tactic: 0x4963fb96b4067e81
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x4963fb96b4067e81 Time: 0.381952
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x49ecad9da64c487b
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x49ecad9da64c487b Time: 0.401408
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v0 Tactic: 0x4c5584586319b832
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x4c5584586319b832 Time: 0.362837
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x5c38385751ccb068
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x5c38385751ccb068 Time: 0.326656
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v0 Tactic: 0x632674f65e3422ae
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x632674f65e3422ae Time: 0.371712
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v0 Tactic: 0x8d563cb6e2bd3e46
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x8d563cb6e2bd3e46 Time: 0.373077
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1 Tactic: 0xa4bca1d50cb9f7ec
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xa4bca1d50cb9f7ec Time: 0.329387
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v0 Tactic: 0xbd90052d8b47dde9
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xbd90052d8b47dde9 Time: 0.351573
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1 Tactic: 0xd00838485d937dc1
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xd00838485d937dc1 Time: 0.337237
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v0 Tactic: 0xef1674e9526bef07
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xef1674e9526bef07 Time: 0.376491
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v0 Tactic: 0xf462d2631d68e4d5
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xf462d2631d68e4d5 Time: 0.367616
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1 Tactic: 0xfa4db728b7a121ee
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xfa4db728b7a121ee Time: 0.406528
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v0 Tactic: 0xfac2e123a5eb1714
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xfac2e123a5eb1714 Time: 0.377856
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xff7bc8e660bee75d
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xff7bc8e660bee75d Time: 0.316928
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0xff7bc8e660bee75d Time: 0.316928
[08/18/2022-12:25:07] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CublasConvolution Tactic: 0x0000000000000001
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(3136,1,3136,3136) -> Float(1024,1,1024,1024) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] (CublasConvolution)
[08/18/2022-12:25:07] [TRT] [V] CublasConvolution has no valid tactics for this config, skipping
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] (CaskConvolution)
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x35f071de80e3b3c4
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x35f071de80e3b3c4 Time: 0.186197
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x5c024b37b78e77c0
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x5c024b37b78e77c0 Time: 0.136988
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x80f932c0b8ce5940
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x80f932c0b8ce5940 Time: 0.138581
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Set Tactic Name: maxwell_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x9961ac24fc07a1df
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x9961ac24fc07a1df Time: 0.186709
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x5c024b37b78e77c0 Time: 0.136988
[08/18/2022-12:25:07] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5c024b37b78e77c0
[08/18/2022-12:25:07] [TRT] [V] =============== Computing costs for 
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(1024,1,1,1) -> Float(10,1,1,1) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 13) [Matrix Multiply] (CudaDepthwiseConvolution)
[08/18/2022-12:25:07] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 13) [Matrix Multiply] (FusedConvActConvolution)
[08/18/2022-12:25:07] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 13) [Matrix Multiply] (CudnnConvolution)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.0635129
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000001 Time: 0.0084224
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000002 Time: 0.305152
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000004 Time: 0.368117
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000005 Time: 0.111378
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x0000000000000001 Time: 0.0084224
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 13) [Matrix Multiply] (CublasConvolution)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00786654
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000001 Time: 0.0114688
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00786654
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 13) [Matrix Multiply] (CaskConvolution)
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1 Tactic: 0x0ebe499388e08286
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0ebe499388e08286 Time: 0.090112
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v0 Tactic: 0x1792ed6b0f1ea883
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x1792ed6b0f1ea883 Time: 0.0644551
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x3e787008e11a6129
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x3e787008e11a6129 Time: 0.0989867
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1 Tactic: 0x474c9edd1ecfbbba
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x474c9edd1ecfbbba Time: 0.0658204
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v0 Tactic: 0x4963fb96b4067e81
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x4963fb96b4067e81 Time: 0.0959147
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x49ecad9da64c487b
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x49ecad9da64c487b Time: 0.0955733
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v0 Tactic: 0x4c5584586319b832
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x4c5584586319b832 Time: 0.0820565
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x5c38385751ccb068
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x5c38385751ccb068 Time: 0.0715456
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v0 Tactic: 0x632674f65e3422ae
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x632674f65e3422ae Time: 0.0672427
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v0 Tactic: 0x8d563cb6e2bd3e46
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x8d563cb6e2bd3e46 Time: 0.0982187
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1 Tactic: 0xa4bca1d50cb9f7ec
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xa4bca1d50cb9f7ec Time: 0.0688128
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v0 Tactic: 0xbd90052d8b47dde9
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xbd90052d8b47dde9 Time: 0.0729088
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1 Tactic: 0xd00838485d937dc1
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xd00838485d937dc1 Time: 0.0730453
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v0 Tactic: 0xef1674e9526bef07
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xef1674e9526bef07 Time: 0.0891733
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v0 Tactic: 0xf462d2631d68e4d5
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xf462d2631d68e4d5 Time: 0.0836267
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1 Tactic: 0xfa4db728b7a121ee
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xfa4db728b7a121ee Time: 0.0948907
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v0 Tactic: 0xfac2e123a5eb1714
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xfac2e123a5eb1714 Time: 0.0946347
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 13) [Matrix Multiply] Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xff7bc8e660bee75d
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0xff7bc8e660bee75d Time: 0.0641707
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0xff7bc8e660bee75d Time: 0.0641707
[08/18/2022-12:25:07] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CublasConvolution Tactic: 0x0000000000000000
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(1024,1,1024,1024) -> Float(10,1,10,10) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 13) [Matrix Multiply] (CublasConvolution)
[08/18/2022-12:25:07] [TRT] [V] CublasConvolution has no valid tactics for this config, skipping
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 13) [Matrix Multiply] (CaskConvolution)
[08/18/2022-12:25:07] [TRT] [V] CaskConvolution has no valid tactics for this config, skipping
[08/18/2022-12:25:07] [TRT] [V] =============== Computing costs for 
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(10,1,1,1) -> Float(10,1) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: reshape_after_(Unnamed Layer* 13) [Matrix Multiply] (Shuffle)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000000 Time: 0.00285657
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x0000000000000001 Time: 0.0169301
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x0000000000000000 Time: 0.00285657
[08/18/2022-12:25:07] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000
[08/18/2022-12:25:07] [TRT] [V] =============== Computing costs for 
[08/18/2022-12:25:07] [TRT] [V] *************** Autotuning format combination: Float(10,1) -> Float(1,1), Int32(1,1) ***************
[08/18/2022-12:25:07] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 16) [Softmax] + (Unnamed Layer* 17) [TopK] (LogSoftmaxTopK)
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000000003e9 Time: 0.00264323
[08/18/2022-12:25:07] [TRT] [V] Tactic: 0x00000000000003ea Time: 0.0150187
[08/18/2022-12:25:07] [TRT] [V] Fastest Tactic: 0x00000000000003e9 Time: 0.00264323
[08/18/2022-12:25:07] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: LogSoftmaxTopK Tactic: 0x00000000000003e9
[08/18/2022-12:25:07] [TRT] [V] Adding reformat layer: Reformatted Output Tensor 0 to (Unnamed Layer* 2) [Pooling] ((Unnamed Layer* 2) [Pooling]_output) from Float(6272,196,14,1) to Float(6272,1,448,32)
[08/18/2022-12:25:07] [TRT] [V] Adding reformat layer: Reformatted Input Tensor 0 to (Unnamed Layer* 5) [Pooling] ((Unnamed Layer* 4) [Activation]_output) from Float(12544,1,896,64) to Float(12544,196,14,1)
[08/18/2022-12:25:07] [TRT] [V] Formats and tactics selection completed in 1.38909 seconds.
[08/18/2022-12:25:07] [TRT] [V] After reformat layers: 11 layers
[08/18/2022-12:25:07] [TRT] [V] Pre-optimized block assignment.
[08/18/2022-12:25:07] [TRT] [V] Block size 802816
[08/18/2022-12:25:07] [TRT] [V] Block size 200704
[08/18/2022-12:25:07] [TRT] [V] Block size 401408
[08/18/2022-12:25:07] [TRT] [V] Block size 100352
[08/18/2022-12:25:07] [TRT] [V] Block size 4
[08/18/2022-12:25:07] [TRT] [V] Block size 512
[08/18/2022-12:25:07] [TRT] [V] Block size 4
[08/18/2022-12:25:07] [TRT] [V] Block size 512
[08/18/2022-12:25:07] [TRT] [V] Block size 32768
[08/18/2022-12:25:07] [TRT] [V] Block size 200704
[08/18/2022-12:25:07] [TRT] [V] Block size 401408
[08/18/2022-12:25:07] [TRT] [V] Block size 6442450944
[08/18/2022-12:25:07] [TRT] [V] Total Activation Memory: 6444592136
[08/18/2022-12:25:07] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v0 Tactic: 0x632674f65e3422ae
[08/18/2022-12:25:07] [TRT] [V] (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Set Tactic Name: maxwell_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x80f932c0b8ce5940
[08/18/2022-12:25:07] [TRT] [V] Layer: (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation] Host Persistent: 1664 Device Persistent: 5120 Scratch Memory: 0
[08/18/2022-12:25:07] [TRT] [V] Layer: (Unnamed Layer* 2) [Pooling] Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/18/2022-12:25:07] [TRT] [V] Layer: Reformatting CopyNode for Output Tensor 0 to (Unnamed Layer* 2) [Pooling] Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/18/2022-12:25:07] [TRT] [V] Layer: (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation] Host Persistent: 1664 Device Persistent: 1536 Scratch Memory: 0
[08/18/2022-12:25:07] [TRT] [V] Layer: Reformatting CopyNode for Input Tensor 0 to (Unnamed Layer* 5) [Pooling] Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/18/2022-12:25:07] [TRT] [V] Layer: (Unnamed Layer* 5) [Pooling] Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/18/2022-12:25:07] [TRT] [V] Layer: (Unnamed Layer* 6) [Shuffle] + reshape_before_(Unnamed Layer* 8) [Matrix Multiply] Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/18/2022-12:25:07] [TRT] [V] Layer: (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation] Host Persistent: 340 Device Persistent: 0 Scratch Memory: 0
[08/18/2022-12:25:07] [TRT] [V] Layer: (Unnamed Layer* 13) [Matrix Multiply] Host Persistent: 340 Device Persistent: 0 Scratch Memory: 0
[08/18/2022-12:25:07] [TRT] [V] Layer: reshape_after_(Unnamed Layer* 13) [Matrix Multiply] Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/18/2022-12:25:07] [TRT] [V] Layer: (Unnamed Layer* 16) [Softmax] + (Unnamed Layer* 17) [TopK] Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
[08/18/2022-12:25:07] [TRT] [I] Total Host Persistent Memory: 4032
[08/18/2022-12:25:07] [TRT] [I] Total Device Persistent Memory: 6656
[08/18/2022-12:25:07] [TRT] [I] Total Scratch Memory: 0
[08/18/2022-12:25:07] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 24 MiB, GPU 434 MiB
[08/18/2022-12:25:07] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.056814ms to assign 3 blocks to 11 nodes requiring 1204228 bytes.
[08/18/2022-12:25:07] [TRT] [V] Optimized block assignment.
[08/18/2022-12:25:07] [TRT] [V] Block size 802816
[08/18/2022-12:25:07] [TRT] [V] Block size 401408
[08/18/2022-12:25:07] [TRT] [V] Block size 4
[08/18/2022-12:25:07] [TRT] [I] Total Activation Memory: 1204228
[08/18/2022-12:25:07] [TRT] [V] Disabling unused tactic source: CUDNN
[08/18/2022-12:25:07] [TRT] [V] Using cublas as a tactic source
[08/18/2022-12:25:07] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 928, GPU 1304 (MiB)
[08/18/2022-12:25:07] [TRT] [V] Engine generation completed in 1.74407 seconds.
[08/18/2022-12:25:07] [TRT] [V] Deleting timing cache: 33 entries, served 0 hits since creation.
[08/18/2022-12:25:07] [TRT] [V] Engine Layer Information:
Layer(CaskConvolution): (Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation], Tactic: 0x632674f65e3422ae, inputT0[Float(-2,1,28,28)] -> (Unnamed Layer* 1) [Activation]_output[Float(-2,32,28,28)]
Layer(TiledPooling): (Unnamed Layer* 2) [Pooling], Tactic: 0x0000000000630101, (Unnamed Layer* 1) [Activation]_output[Float(-2,32,28,28)] -> Reformatted Output Tensor 0 to (Unnamed Layer* 2) [Pooling][Float(-2,32,14,14)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to (Unnamed Layer* 2) [Pooling], Tactic: 0x0000000000000000, Reformatted Output Tensor 0 to (Unnamed Layer* 2) [Pooling][Float(-2,32,14,14)] -> (Unnamed Layer* 2) [Pooling]_output[Float(-2,32,14,14)]
Layer(CaskConvolution): (Unnamed Layer* 3) [Convolution] + (Unnamed Layer* 4) [Activation], Tactic: 0x80f932c0b8ce5940, (Unnamed Layer* 2) [Pooling]_output[Float(-2,32,14,14)] -> (Unnamed Layer* 4) [Activation]_output[Float(-2,64,14,14)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to (Unnamed Layer* 5) [Pooling], Tactic: 0x0000000000000000, (Unnamed Layer* 4) [Activation]_output[Float(-2,64,14,14)] -> Reformatted Input Tensor 0 to (Unnamed Layer* 5) [Pooling][Float(-2,64,14,14)]
Layer(TiledPooling): (Unnamed Layer* 5) [Pooling], Tactic: 0x0000000000630101, Reformatted Input Tensor 0 to (Unnamed Layer* 5) [Pooling][Float(-2,64,14,14)] -> (Unnamed Layer* 5) [Pooling]_output[Float(-2,64,7,7)]
Layer(NoOp): (Unnamed Layer* 6) [Shuffle] + reshape_before_(Unnamed Layer* 8) [Matrix Multiply], Tactic: 0x0000000000000000, (Unnamed Layer* 5) [Pooling]_output[Float(-2,64,7,7)] -> reshape_before_(Unnamed Layer* 8) [Matrix Multiply]_out_region[Float(-2,3136,1,1)]
Layer(CublasConvolution): (Unnamed Layer* 8) [Matrix Multiply] + (Unnamed Layer* 11) [Activation], Tactic: 0x0000000000000001, reshape_before_(Unnamed Layer* 8) [Matrix Multiply]_out_region[Float(-2,3136,1,1)] -> (Unnamed Layer* 11) [Activation]_out_tensor[Float(-2,1024,1,1)]
Layer(CublasConvolution): (Unnamed Layer* 13) [Matrix Multiply], Tactic: 0x0000000000000000, (Unnamed Layer* 11) [Activation]_out_tensor[Float(-2,1024,1,1)] -> (Unnamed Layer* 13) [Matrix Multiply]_out_region[Float(-2,10,1,1)]
Layer(NoOp): reshape_after_(Unnamed Layer* 13) [Matrix Multiply], Tactic: 0x0000000000000000, (Unnamed Layer* 13) [Matrix Multiply]_out_region[Float(-2,10,1,1)] -> (Unnamed Layer* 15) [ElementWise]_output[Float(-2,10)]
Layer(LogSoftmaxTopK): (Unnamed Layer* 16) [Softmax] + (Unnamed Layer* 17) [TopK], Tactic: 0x00000000000003e9, (Unnamed Layer* 15) [ElementWise]_output[Float(-2,10)] -> (Unnamed Layer* 17) [TopK]_output_1[Float(-2,1)], (Unnamed Layer* 17) [TopK]_output_2[Int32(-2,1)]
[08/18/2022-12:25:07] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +12, GPU +13, now: CPU 12, GPU 13 (MiB)
[08/18/2022-12:25:07] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[08/18/2022-12:25:07] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[08/18/2022-12:25:07] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 928, GPU 1264 (MiB)
[08/18/2022-12:25:07] [TRT] [I] Loaded engine size: 12 MiB
[08/18/2022-12:25:07] [TRT] [V] Using cublas as a tactic source
[08/18/2022-12:25:07] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 928, GPU 1290 (MiB)
[08/18/2022-12:25:07] [TRT] [V] Deserialization required 5713 microseconds.
[08/18/2022-12:25:07] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12, now: CPU 0, GPU 12 (MiB)
[08/18/2022-12:25:07] [TRT] [V] Using cublas as a tactic source
[08/18/2022-12:25:07] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 928, GPU 1290 (MiB)
[08/18/2022-12:25:07] [TRT] [V] Total per-runner device persistent memory is 6656
[08/18/2022-12:25:07] [TRT] [V] Total per-runner host persistent memory is 4032
[08/18/2022-12:25:07] [TRT] [V] Allocated activation device memory of size 1204736
[08/18/2022-12:25:07] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1, now: CPU 0, GPU 13 (MiB)
Succeeded building serialized engine!
Succeeded building engine!
Bind[ 0]:i[ 0]-> DataType.FLOAT (-1, 1, 28, 28) (1, 1, 28, 28) inputT0
Bind[ 1]:o[ 0]-> DataType.INT32 (-1, 1) (1, 1) (Unnamed Layer* 17) [TopK]_output_2
inputT0
[[[[ 0.055 -0.731  0.653 -0.783 -0.202  0.936 -0.62   0.504 -0.445  0.278  0.283 -0.274 -0.585 -0.169 -0.567 -0.28   0.731 -0.802 -0.167 -0.906 -0.296 -0.17  -0.394  0.621 -0.407 -0.667  0.77
     0.609]
   [-0.764 -0.881  0.327 -0.636 -0.467  0.734 -0.836  0.979 -0.454  0.826  0.027  0.151 -0.472  0.877 -0.359  0.564  0.239 -0.59  -0.549  0.986  0.538  0.28   0.385  0.005 -0.261 -0.234 -0.604
    -0.504]
   [-0.578 -0.547 -0.008 -0.926  0.466  0.017  0.001 -0.952  0.106  0.123 -0.13  -0.434 -0.826 -0.65   0.457 -0.041  0.723  0.242  0.773  0.501  0.971 -0.448 -0.197 -0.02   0.665 -0.199  0.943
    -0.795]
   [-0.876 -0.827  0.924 -0.764 -0.265 -0.884  0.653  0.078  0.909  0.72  -0.728 -0.18   0.639  0.334 -0.696  0.469  0.438 -0.338 -0.504 -0.69   0.315 -0.29   0.822  0.06   0.88  -0.47   0.211
    -0.603]
   [ 0.075  0.491  0.36  -0.196 -0.884 -0.401  0.613 -0.549  0.481  0.709  0.422  0.67  -0.944  0.326  0.888 -0.695 -0.866  0.303  0.526 -0.643 -0.892 -0.989  0.524 -0.166 -0.105 -0.309  0.734
    -0.477]
   [-0.376 -0.429 -0.156 -0.753  0.461 -0.81   0.894 -0.643 -0.349 -0.882 -0.772  0.909  0.779 -0.736  0.786 -0.844 -0.159 -0.064  0.359  0.632 -0.317 -0.095 -0.526 -0.243 -0.425 -0.888  0.575
    -0.498]
   [-0.558  0.759 -0.51   0.486  0.463  0.257  0.618 -0.833 -0.049  0.737  0.933 -0.546 -0.882 -0.095 -0.506  0.07   0.938  0.62  -0.812  0.659  0.92  -0.943  0.321  0.095 -0.973  0.205 -0.858
    -0.056]
   [ 0.218  0.076 -0.458 -0.878  0.255 -0.635  0.39  -0.659  0.747  0.915  0.226  0.186 -0.672  0.368 -0.676  0.345  0.466 -0.708  0.208  0.563 -0.376  0.887  0.307 -0.357 -0.631  0.665 -0.372
    -0.054]
   [ 0.328 -0.467  0.406 -0.479  0.19   0.809 -0.029  0.924  0.58  -0.818 -0.805 -0.945 -0.532  0.615 -0.828 -0.569 -0.043  0.551 -0.328 -0.534 -0.922  0.601 -0.735  0.953  0.419  0.674  0.877
     0.084]
   [ 0.377 -0.172  0.697 -0.724 -0.691  0.536  0.272  0.276 -0.452  0.678 -0.316  0.057 -0.596 -0.685  0.765  0.535 -0.674  0.241  0.339  0.88  -0.044 -0.108 -0.875  0.365  0.401 -0.124 -0.35
    -0.049]
   [ 0.644 -0.125 -0.833  0.962 -0.975  0.652 -0.189  0.819 -0.181 -0.224  0.827  0.801 -0.258  0.965  0.531 -0.216 -0.112  0.402 -0.003 -0.029 -0.145 -0.722 -0.167  0.537 -0.197 -0.358 -0.062
    -0.913]
   [-0.255 -0.971 -0.824 -0.472  0.704  0.66  -0.004 -0.733 -0.404 -0.939 -0.061 -0.57   0.241  0.222  0.283  0.277  0.387 -0.326 -0.264  0.194  0.051  0.471  0.723 -0.29  -0.945 -0.022 -0.002
    -0.586]
   [ 0.719 -0.108  0.731  0.147  0.088 -0.793 -0.148 -0.628  0.255 -0.37   0.45  -0.275  0.517 -0.38   0.341  0.464  0.445  0.478 -0.898  0.597  0.847  0.866  0.357  0.336 -0.04   0.382 -0.248
     0.634]
   [ 0.088 -0.103 -0.567  0.075 -0.798 -0.258 -0.401  0.633 -0.84  -0.446 -0.69   0.374 -0.021  0.82  -0.623 -0.544  0.188 -0.3    0.105  0.526  0.296 -0.049 -0.744  0.932  0.562 -0.37  -0.009
     0.4  ]
   [-0.189 -0.743  0.143 -0.456  0.626  0.929 -0.486  0.5    0.148  0.956  0.468  0.897 -0.708  0.19  -0.043  0.362  0.473 -0.504 -0.195  0.74   0.158  0.831 -0.308 -0.29   0.47  -0.538 -0.848
    -0.672]
   [-0.582  0.37  -0.51   0.392 -0.215  0.416 -0.529  0.873 -0.504 -0.088  0.137 -0.183 -0.227 -0.186 -0.116  0.434 -0.776 -0.144  0.497  0.597  0.081 -0.402  0.026 -0.523  0.071  0.919  0.447
    -0.998]
   [-0.625 -0.799 -0.202 -0.087 -0.639 -0.279  0.449 -0.623 -0.693  0.102 -0.158  0.767 -0.141  0.547  0.523  0.177  0.704 -0.879  0.044 -0.672 -0.907 -0.007  0.702 -0.415 -0.592 -0.427  0.114
    -0.225]
   [-0.648  0.941 -0.097 -0.729 -0.377  0.358  0.286 -0.888 -0.111 -0.587  0.997  0.692 -0.382  0.705 -0.602 -0.454  0.501  0.657  0.777  0.355 -0.66  -0.325  0.914  0.516 -0.263 -0.122 -0.239
     0.332]
   [-0.795 -0.576 -0.401  0.652  0.67   0.116  0.153 -0.083 -0.326 -0.225 -0.138 -0.337  0.298 -0.044  0.738  0.877  0.157  0.472  0.553  0.554 -0.784 -0.508 -0.549 -0.565  0.715  0.819 -0.641
    -0.691]
   [ 0.213  0.615  0.108 -0.635  0.107  0.247 -0.11  -0.285 -0.875 -0.233 -0.172  0.194 -0.149  0.562 -0.365  0.087  0.915 -0.556 -0.941  0.406  0.17  -0.848 -0.274  0.951  0.325 -0.252 -0.922
     0.582]
   [-0.499  0.152  0.703  0.323 -0.316 -0.808 -0.588 -0.766 -0.489  0.523 -0.911 -0.25  -0.335  0.456  0.26  -0.876  0.74  -0.647  0.434 -0.824  0.261  0.398  0.972 -0.898 -0.58  -0.165  0.23
    -0.822]
   [ 0.49   0.186  0.269 -0.315 -0.641 -0.943  0.985  0.31   0.222  0.457 -0.939 -0.356  0.553  0.927  0.393 -0.946  0.658  0.716 -0.611  0.581  0.763  0.778  0.744  0.003 -0.511  0.35   0.463
    -0.97 ]
   [ 0.894  0.439  0.352  0.938 -0.107 -0.054  0.077 -0.477  0.835 -0.656  0.515  0.541  0.662  0.241 -0.606  0.807  0.029  0.954  0.335 -0.139  0.994 -0.475  0.415 -0.083  0.14   0.592 -0.926
     0.761]
   [ 0.284 -0.045  0.439  0.146 -0.599 -0.168  0.935 -0.779  0.025  0.591  0.332 -0.936 -0.995 -0.277  0.863 -0.538 -0.011  0.169 -0.778  0.225  0.932 -0.522 -0.031 -0.561  0.331  0.485 -0.401
     0.606]
   [-0.802  0.071  0.313  0.541 -0.163  0.246 -0.04   0.718 -0.467  0.321  0.852  0.146 -0.391 -0.962 -0.38   0.892 -0.476 -0.888  0.925 -0.118  0.264  0.93   0.124  0.442  0.876 -0.537  0.135
     0.687]
   [ 0.249  0.766  0.977 -0.902 -0.96  -0.671  0.729 -0.553 -0.545 -0.277  0.35   0.144  0.242 -0.811  0.238 -0.138  0.085 -0.002  0.918  0.242  0.064 -0.324  0.935  0.668 -0.118 -0.559  0.464
    -0.228]
   [ 0.36  -0.243 -0.009  0.249 -0.042 -0.501 -0.726 -0.236 -0.266  0.004 -0.719  0.803  0.916  0.599 -0.678  0.899 -0.796  0.117 -0.956  0.89  -0.743  0.694  0.543 -0.079 -0.35   0.975 -0.845
     0.893]
   [ 0.052 -0.642  0.265  0.175  0.004  0.772 -0.093 -0.583  0.932 -0.506 -0.094 -0.186  0.385 -0.936 -0.766  0.213 -0.386 -0.164 -0.932 -0.158 -0.984 -0.509 -0.528  0.113  0.535 -0.316 -0.536
    -0.377]]]]
(Unnamed Layer* 17) [TopK]_output_2
[[4]]
