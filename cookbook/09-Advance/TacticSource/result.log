[09/07/2022-03:05:42] [TRT] [I] [MemUsageChange] Init CUDA: CPU +188, GPU +0, now: CPU 209, GPU 400 (MiB)
[09/07/2022-03:05:43] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +6, GPU +2, now: CPU 234, GPU 415 (MiB)
[09/07/2022-03:05:43] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +256, GPU +106, now: CPU 515, GPU 521 (MiB)
[09/07/2022-03:05:43] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +113, GPU +46, now: CPU 628, GPU 567 (MiB)
[09/07/2022-03:05:43] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[09/07/2022-03:05:45] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[09/07/2022-03:05:45] [TRT] [I] Total Host Persistent Memory: 4560
[09/07/2022-03:05:45] [TRT] [I] Total Device Persistent Memory: 5120
[09/07/2022-03:05:45] [TRT] [I] Total Scratch Memory: 0
[09/07/2022-03:05:45] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 24 MiB, GPU 7095 MiB
[09/07/2022-03:05:45] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.019046ms to assign 3 blocks to 9 nodes requiring 250884 bytes.
[09/07/2022-03:05:45] [TRT] [I] Total Activation Memory: 250884
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 922, GPU 707 (MiB)
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +12, GPU +13, now: CPU 12, GPU 13 (MiB)
[09/07/2022-03:05:45] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[09/07/2022-03:05:45] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 922, GPU 667 (MiB)
[09/07/2022-03:05:45] [TRT] [I] Loaded engine size: 12 MiB
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 922, GPU 691 (MiB)
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12, now: CPU 0, GPU 12 (MiB)
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 922, GPU 691 (MiB)
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 12 (MiB)
[09/07/2022-03:05:45] [TRT] [I] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.

[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 910, GPU 667 (MiB)
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 934, GPU 677 (MiB)
[09/07/2022-03:05:45] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[09/07/2022-03:05:45] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[09/07/2022-03:05:45] [TRT] [I] Total Host Persistent Memory: 4560
[09/07/2022-03:05:45] [TRT] [I] Total Device Persistent Memory: 5120
[09/07/2022-03:05:45] [TRT] [I] Total Scratch Memory: 0
[09/07/2022-03:05:45] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 24 MiB, GPU 7095 MiB
[09/07/2022-03:05:45] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.019876ms to assign 3 blocks to 9 nodes requiring 250884 bytes.
[09/07/2022-03:05:45] [TRT] [I] Total Activation Memory: 250884
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 935, GPU 699 (MiB)
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +12, GPU +13, now: CPU 12, GPU 13 (MiB)
[09/07/2022-03:05:45] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[09/07/2022-03:05:45] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[09/07/2022-03:05:45] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.

[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 935, GPU 667 (MiB)
[09/07/2022-03:05:45] [TRT] [I] Loaded engine size: 12 MiB
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 935, GPU 691 (MiB)
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12, now: CPU 0, GPU 12 (MiB)
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 935, GPU 691 (MiB)
[09/07/2022-03:05:45] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 12 (MiB)
Bind[ 0]:i[ 0]-> DataType.FLOAT (-1, 1, 28, 28) (1, 1, 28, 28) inputT0
Bind[ 1]:o[ 0]-> DataType.INT32 (-1, 1) (1, 1) (Unnamed Layer* 17) [TopK]_output_2
Timing:1.354694 ms
inputT0
[[[[ 0.321 -0.305 -0.455  0.461 -0.233 -0.366 -0.119 -0.425  0.358 -0.852  0.006 -0.024 -0.956 -0.908 -0.954  0.881  0.458 -0.238  0.151 -0.717 -0.845 -0.063  0.048  0.906 -0.385 -0.65  -0.658
    -0.027]
   [-0.749 -0.649  0.047 -0.94  -0.659 -0.64   0.395 -0.092  0.351  0.658  0.831  0.275  0.395  0.55  -0.532  0.892 -0.707  0.153 -0.017  0.462  0.45  -0.259  0.87   0.951 -0.343  0.266  0.467
    -0.95 ]
   [-0.164 -0.733 -0.111  0.395 -0.152 -0.899  0.983 -0.746 -0.291  0.5   -0.418  0.174  0.95   0.759 -0.735 -0.269 -0.517 -0.359 -0.719  0.972 -0.527  0.65  -0.321  0.228  0.152 -0.443 -0.615
     0.226]
   [ 0.217 -0.092  0.432  0.921 -0.839 -0.348  0.089  0.166  0.208 -0.956 -0.209  0.863  0.168  0.316 -0.63   0.082 -0.755  0.378  0.073 -0.57  -0.16  -0.549 -0.878 -0.3   -0.643  0.486 -0.144
     0.355]
   [-0.272 -0.955  0.271  0.63   0.218  0.967 -0.015  0.933  0.462 -0.309 -0.593 -0.14  -0.561 -0.38  -0.71   0.479 -0.11   0.967 -0.657  0.293 -0.352  0.948 -0.214 -0.854  0.998 -0.383 -0.328
     0.258]
   [-0.221  0.567 -0.135 -0.223 -0.635 -0.959 -0.861  0.771  0.398 -0.699  0.609  0.674  0.673  0.973 -0.359  0.698  0.922 -0.065  0.872  0.675  0.002 -0.641 -0.181 -0.468  0.836 -0.422  0.358
    -0.749]
   [ 0.952  0.987  0.522 -0.216 -0.061  0.499  0.679 -0.977  0.147  0.29  -0.797 -0.474 -0.044 -0.551  0.17  -0.509  0.869 -0.921 -0.698  0.677  0.557  0.006  0.849  0.7   -0.564  0.531 -0.59
    -0.833]
   [-0.532  0.521  0.315  0.951  0.438  0.936  0.403 -0.995  0.705 -0.471  0.411 -0.921  0.626  0.881  0.364 -0.703  0.148 -0.741  0.594  0.592  0.687 -0.746  0.037 -0.031 -0.485 -0.79  -0.51
    -0.749]
   [-0.497 -0.947 -0.03   0.359 -0.146 -0.259 -0.32   0.319  0.028  0.53  -0.809  0.152  0.461 -0.849  0.808  0.784  0.605 -0.298  0.935  0.796  0.865  0.392  0.044 -0.588  0.432 -0.483  0.808
     0.438]
   [-0.372  0.587 -0.402 -0.307  0.679  0.273  0.124  0.964  0.275 -0.467 -0.08  -0.058  0.722  0.381  0.721 -0.331 -0.856  0.197  0.374  0.88  -0.43  -0.557 -0.886 -0.699  0.239 -0.189  0.861
     0.752]
   [-0.365  0.514 -0.809  0.411 -0.324 -0.883 -0.976 -0.866 -0.642 -0.727  0.726  0.849 -0.918  0.417  0.12   0.92   0.017  0.227 -0.557 -0.666 -0.017 -0.197 -0.74   0.732 -0.126  0.643  0.766
    -0.741]
   [ 0.009 -0.695  0.923 -0.106 -0.078  0.935 -0.173  0.346 -0.233 -0.066 -0.388 -0.257 -0.282  0.618  0.404 -0.814  0.53   0.75  -0.551  0.486 -0.555  0.133  0.544 -0.704  0.982 -0.729 -0.059
    -0.968]
   [ 0.615 -0.277 -0.051 -0.397  0.816  0.861  0.532  0.875 -0.24  -0.976 -0.647  0.784 -0.304  0.261  0.746 -0.305  0.044  0.086  0.345 -0.831  0.866  0.145 -0.816  0.414  0.419 -0.732  0.176
    -0.888]
   [ 0.426 -0.634 -0.105  0.043  0.348  0.389 -0.235  0.518 -0.24  -0.949  0.712 -0.813 -0.608 -0.074 -0.076 -0.246  0.736  0.566  0.756  0.4   -0.962 -0.683 -0.716  0.083 -0.492 -0.939  0.413
     0.579]
   [ 0.63  -0.449  0.735  0.085 -0.497  0.521  0.636 -0.399  0.441  0.25  -0.114  0.534  0.099  0.874 -0.323 -0.097 -0.234  0.586 -0.925 -0.114 -0.894  0.746 -0.64   0.127 -0.067 -0.998 -0.253
     0.53 ]
   [-0.144  0.825  0.014  0.116 -0.053  0.077  0.377 -0.078  0.985  0.505  0.981 -0.744 -0.871 -0.299  0.788 -0.87  -0.205 -0.287 -0.534  0.396 -0.215  0.579  0.686  0.958  0.337  0.097 -0.552
     0.124]
   [ 0.186  0.191 -0.702  0.586 -0.273 -0.544 -0.235  0.667 -0.003  0.013  0.322  0.25  -0.144 -0.781  0.74   0.355  0.246  0.663 -0.45  -0.762  0.962 -0.005  0.587 -0.977  0.295 -0.945 -0.69
    -0.814]
   [-0.142  0.664 -0.725  0.248  0.114 -0.826  0.038  0.65   0.633  0.673  0.178  0.146  0.057  0.982 -0.405  0.34  -0.147 -0.215  0.55   0.331 -0.927 -0.459 -0.06  -0.263 -0.372 -0.537  0.874
    -0.407]
   [-0.296 -0.836  0.836 -0.726  0.13  -0.139 -0.964 -0.939 -0.814  0.701  0.05   0.138 -0.61   0.589 -0.538 -0.38  -0.77   0.298  0.547 -0.898 -0.114  0.178  0.384  0.432 -0.25  -0.305  0.198
    -0.992]
   [-0.287  0.746 -0.731 -0.811  0.402  0.95   0.746 -0.899 -0.127  0.175 -0.493 -0.296 -0.274  0.191 -0.297  0.448  0.273 -0.522 -0.59   0.007 -0.552  0.168  0.181 -0.543 -0.508 -0.965  0.125
     0.11 ]
   [-0.449 -0.467 -0.532 -0.091 -0.186  0.439  0.926  0.349 -0.397 -0.089  0.371 -0.747 -0.174 -0.412  0.823  0.229 -0.759  0.79   0.042 -0.351  0.992 -0.733  0.301  0.616 -0.189 -0.112  0.378
    -0.575]
   [-0.886  0.058  0.19  -0.154  0.549 -0.046 -0.974 -0.928  0.902 -0.59  -0.564 -0.32  -0.215 -0.871  0.473  0.486 -0.477  0.649  0.577  0.867 -0.159 -0.45   0.481  0.649  0.338  0.047  0.416
    -0.083]
   [ 0.486  0.571 -0.479  0.024  0.201  0.391 -0.006  0.393 -0.527 -0.707 -0.178 -0.7   -0.952  0.922  0.431  0.879 -0.44   0.147  0.873  0.456  0.524  0.658  0.233 -0.518  0.999 -0.861  0.764
     0.264]
   [-0.742 -0.006 -0.38  -0.856 -0.256  0.108  0.505 -0.764 -0.223  0.671  0.334 -0.727 -0.953  0.782 -0.048 -0.436 -0.216  0.076  0.825 -0.796 -0.877 -0.432 -0.221  0.067  0.75  -0.474  0.062
    -0.03 ]
   [-0.131 -0.245  0.241 -0.469 -0.125  0.881 -0.148  0.438 -0.407 -0.942  0.75  -0.382 -0.75  -0.524 -0.163  0.845  0.626 -0.882 -0.505  0.886  0.287  0.314  0.403 -0.298  0.857 -0.018 -0.715
     0.467]
   [ 0.112 -0.164  0.406 -0.515  0.196 -0.491 -0.56   0.913 -0.248 -0.198  0.748 -0.796  0.174  0.217 -0.988 -0.973  0.701  0.709 -0.727  0.021  0.616 -0.933  0.967  0.581 -0.363 -0.157 -0.947
    -0.718]
   [ 0.924  0.922 -0.416  0.963  0.463  0.392  0.455 -0.256 -0.545  0.996  0.359  0.694 -0.745  0.35   0.122  0.413 -0.09   0.278 -0.747 -0.278 -0.091  0.408 -0.344  0.165 -0.67  -0.442 -0.296
    -0.952]
   [ 0.343 -0.528  0.429 -0.134 -0.82   0.849  0.039 -0.761 -0.811 -0.413  0.215  0.102 -0.503  0.852 -0.146  0.38   0.75   0.534 -0.994 -0.986 -0.397 -0.958  0.168  0.122  0.233  0.411 -0.129
    -0.301]]]]
(Unnamed Layer* 17) [TopK]_output_2
[[4]]
Bind[ 0]:i[ 0]-> DataType.FLOAT (-1, 1, 28, 28) (1, 1, 28, 28) inputT0
Bind[ 1]:o[ 0]-> DataType.INT32 (-1, 1) (1, 1) (Unnamed Layer* 17) [TopK]_output_2
Timing:1.178980 ms
inputT0
[[[[ 0.321 -0.305 -0.455  0.461 -0.233 -0.366 -0.119 -0.425  0.358 -0.852  0.006 -0.024 -0.956 -0.908 -0.954  0.881  0.458 -0.238  0.151 -0.717 -0.845 -0.063  0.048  0.906 -0.385 -0.65  -0.658
    -0.027]
   [-0.749 -0.649  0.047 -0.94  -0.659 -0.64   0.395 -0.092  0.351  0.658  0.831  0.275  0.395  0.55  -0.532  0.892 -0.707  0.153 -0.017  0.462  0.45  -0.259  0.87   0.951 -0.343  0.266  0.467
    -0.95 ]
   [-0.164 -0.733 -0.111  0.395 -0.152 -0.899  0.983 -0.746 -0.291  0.5   -0.418  0.174  0.95   0.759 -0.735 -0.269 -0.517 -0.359 -0.719  0.972 -0.527  0.65  -0.321  0.228  0.152 -0.443 -0.615
     0.226]
   [ 0.217 -0.092  0.432  0.921 -0.839 -0.348  0.089  0.166  0.208 -0.956 -0.209  0.863  0.168  0.316 -0.63   0.082 -0.755  0.378  0.073 -0.57  -0.16  -0.549 -0.878 -0.3   -0.643  0.486 -0.144
     0.355]
   [-0.272 -0.955  0.271  0.63   0.218  0.967 -0.015  0.933  0.462 -0.309 -0.593 -0.14  -0.561 -0.38  -0.71   0.479 -0.11   0.967 -0.657  0.293 -0.352  0.948 -0.214 -0.854  0.998 -0.383 -0.328
     0.258]
   [-0.221  0.567 -0.135 -0.223 -0.635 -0.959 -0.861  0.771  0.398 -0.699  0.609  0.674  0.673  0.973 -0.359  0.698  0.922 -0.065  0.872  0.675  0.002 -0.641 -0.181 -0.468  0.836 -0.422  0.358
    -0.749]
   [ 0.952  0.987  0.522 -0.216 -0.061  0.499  0.679 -0.977  0.147  0.29  -0.797 -0.474 -0.044 -0.551  0.17  -0.509  0.869 -0.921 -0.698  0.677  0.557  0.006  0.849  0.7   -0.564  0.531 -0.59
    -0.833]
   [-0.532  0.521  0.315  0.951  0.438  0.936  0.403 -0.995  0.705 -0.471  0.411 -0.921  0.626  0.881  0.364 -0.703  0.148 -0.741  0.594  0.592  0.687 -0.746  0.037 -0.031 -0.485 -0.79  -0.51
    -0.749]
   [-0.497 -0.947 -0.03   0.359 -0.146 -0.259 -0.32   0.319  0.028  0.53  -0.809  0.152  0.461 -0.849  0.808  0.784  0.605 -0.298  0.935  0.796  0.865  0.392  0.044 -0.588  0.432 -0.483  0.808
     0.438]
   [-0.372  0.587 -0.402 -0.307  0.679  0.273  0.124  0.964  0.275 -0.467 -0.08  -0.058  0.722  0.381  0.721 -0.331 -0.856  0.197  0.374  0.88  -0.43  -0.557 -0.886 -0.699  0.239 -0.189  0.861
     0.752]
   [-0.365  0.514 -0.809  0.411 -0.324 -0.883 -0.976 -0.866 -0.642 -0.727  0.726  0.849 -0.918  0.417  0.12   0.92   0.017  0.227 -0.557 -0.666 -0.017 -0.197 -0.74   0.732 -0.126  0.643  0.766
    -0.741]
   [ 0.009 -0.695  0.923 -0.106 -0.078  0.935 -0.173  0.346 -0.233 -0.066 -0.388 -0.257 -0.282  0.618  0.404 -0.814  0.53   0.75  -0.551  0.486 -0.555  0.133  0.544 -0.704  0.982 -0.729 -0.059
    -0.968]
   [ 0.615 -0.277 -0.051 -0.397  0.816  0.861  0.532  0.875 -0.24  -0.976 -0.647  0.784 -0.304  0.261  0.746 -0.305  0.044  0.086  0.345 -0.831  0.866  0.145 -0.816  0.414  0.419 -0.732  0.176
    -0.888]
   [ 0.426 -0.634 -0.105  0.043  0.348  0.389 -0.235  0.518 -0.24  -0.949  0.712 -0.813 -0.608 -0.074 -0.076 -0.246  0.736  0.566  0.756  0.4   -0.962 -0.683 -0.716  0.083 -0.492 -0.939  0.413
     0.579]
   [ 0.63  -0.449  0.735  0.085 -0.497  0.521  0.636 -0.399  0.441  0.25  -0.114  0.534  0.099  0.874 -0.323 -0.097 -0.234  0.586 -0.925 -0.114 -0.894  0.746 -0.64   0.127 -0.067 -0.998 -0.253
     0.53 ]
   [-0.144  0.825  0.014  0.116 -0.053  0.077  0.377 -0.078  0.985  0.505  0.981 -0.744 -0.871 -0.299  0.788 -0.87  -0.205 -0.287 -0.534  0.396 -0.215  0.579  0.686  0.958  0.337  0.097 -0.552
     0.124]
   [ 0.186  0.191 -0.702  0.586 -0.273 -0.544 -0.235  0.667 -0.003  0.013  0.322  0.25  -0.144 -0.781  0.74   0.355  0.246  0.663 -0.45  -0.762  0.962 -0.005  0.587 -0.977  0.295 -0.945 -0.69
    -0.814]
   [-0.142  0.664 -0.725  0.248  0.114 -0.826  0.038  0.65   0.633  0.673  0.178  0.146  0.057  0.982 -0.405  0.34  -0.147 -0.215  0.55   0.331 -0.927 -0.459 -0.06  -0.263 -0.372 -0.537  0.874
    -0.407]
   [-0.296 -0.836  0.836 -0.726  0.13  -0.139 -0.964 -0.939 -0.814  0.701  0.05   0.138 -0.61   0.589 -0.538 -0.38  -0.77   0.298  0.547 -0.898 -0.114  0.178  0.384  0.432 -0.25  -0.305  0.198
    -0.992]
   [-0.287  0.746 -0.731 -0.811  0.402  0.95   0.746 -0.899 -0.127  0.175 -0.493 -0.296 -0.274  0.191 -0.297  0.448  0.273 -0.522 -0.59   0.007 -0.552  0.168  0.181 -0.543 -0.508 -0.965  0.125
     0.11 ]
   [-0.449 -0.467 -0.532 -0.091 -0.186  0.439  0.926  0.349 -0.397 -0.089  0.371 -0.747 -0.174 -0.412  0.823  0.229 -0.759  0.79   0.042 -0.351  0.992 -0.733  0.301  0.616 -0.189 -0.112  0.378
    -0.575]
   [-0.886  0.058  0.19  -0.154  0.549 -0.046 -0.974 -0.928  0.902 -0.59  -0.564 -0.32  -0.215 -0.871  0.473  0.486 -0.477  0.649  0.577  0.867 -0.159 -0.45   0.481  0.649  0.338  0.047  0.416
    -0.083]
   [ 0.486  0.571 -0.479  0.024  0.201  0.391 -0.006  0.393 -0.527 -0.707 -0.178 -0.7   -0.952  0.922  0.431  0.879 -0.44   0.147  0.873  0.456  0.524  0.658  0.233 -0.518  0.999 -0.861  0.764
     0.264]
   [-0.742 -0.006 -0.38  -0.856 -0.256  0.108  0.505 -0.764 -0.223  0.671  0.334 -0.727 -0.953  0.782 -0.048 -0.436 -0.216  0.076  0.825 -0.796 -0.877 -0.432 -0.221  0.067  0.75  -0.474  0.062
    -0.03 ]
   [-0.131 -0.245  0.241 -0.469 -0.125  0.881 -0.148  0.438 -0.407 -0.942  0.75  -0.382 -0.75  -0.524 -0.163  0.845  0.626 -0.882 -0.505  0.886  0.287  0.314  0.403 -0.298  0.857 -0.018 -0.715
     0.467]
   [ 0.112 -0.164  0.406 -0.515  0.196 -0.491 -0.56   0.913 -0.248 -0.198  0.748 -0.796  0.174  0.217 -0.988 -0.973  0.701  0.709 -0.727  0.021  0.616 -0.933  0.967  0.581 -0.363 -0.157 -0.947
    -0.718]
   [ 0.924  0.922 -0.416  0.963  0.463  0.392  0.455 -0.256 -0.545  0.996  0.359  0.694 -0.745  0.35   0.122  0.413 -0.09   0.278 -0.747 -0.278 -0.091  0.408 -0.344  0.165 -0.67  -0.442 -0.296
    -0.952]
   [ 0.343 -0.528  0.429 -0.134 -0.82   0.849  0.039 -0.761 -0.811 -0.413  0.215  0.102 -0.503  0.852 -0.146  0.38   0.75   0.534 -0.994 -0.986 -0.397 -0.958  0.168  0.122  0.233  0.411 -0.129
    -0.301]]]]
(Unnamed Layer* 17) [TopK]_output_2
[[2]]
